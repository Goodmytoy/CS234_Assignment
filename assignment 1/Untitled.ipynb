{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comparable-moses",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "### MDP Value Iteration and Policy Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "    P: nested dictionary\n",
    "        From gym.core.Environment\n",
    "        For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "        tuple of the form (probability, nextstate, reward, terminal) where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "            True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS: int\n",
    "        number of states in the environment\n",
    "    nA: int\n",
    "        number of actions in the environment\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "\"\"\"\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    \n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    prev_value_function = value_function.copy() + tol\n",
    "    while np.max(np.abs(value_function - prev_value_function)) >= tol:\n",
    "        prev_value_function = value_function.copy()\n",
    "        for state in range(nS):\n",
    "             for (probability, nextstate, reward, terminal) in P[state][policy[state]]:\n",
    "                value_function[state] = reward + gamma * probability * value_function[nextstate]\n",
    "    ############################\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    for state in range(nS):\n",
    "        q = np.zeros(nA)\n",
    "        for action in range(nA):\n",
    "            probability, nextstate, reward, terminal = P[state][action][0]\n",
    "            q[action] = reward + gamma * probability * value_from_policy[nextstate]\n",
    "        new_policy[state] = np.argmax(q)\n",
    "\n",
    "    ############################\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should call the policy_evaluation() and policy_improvement() methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        tol parameter used in policy_evaluation()\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    prev_policy = policy + 1\n",
    "    while any(policy != prev_policy):\n",
    "        prev_policy = policy.copy()\n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma=gamma, tol=tol)\n",
    "        policy = policy_improvement(P, nS, nA, value_from_policy=value_function, policy=policy, gamma=gamma)\n",
    "\n",
    "    ############################\n",
    "    return value_function, policy\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:    \n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    prev_value_function = value_function.copy() + tol\n",
    "    while np.max(np.abs(value_function - prev_value_function)) >= tol:\n",
    "        prev_value_function = value_function.copy()\n",
    "        for state in range(nS):\n",
    "            value = np.zeros(nA)\n",
    "            for action in range(nA):\n",
    "                probability, nextstate, reward, terminal = P[state][action][0]\n",
    "                value[action] = reward + gamma * probability * value_function[nextstate]\n",
    "            value_function[state] = np.max(value)\n",
    "            policy[state] = np.argmax(value)    \n",
    "    ############################\n",
    "    return value_function, policy\n",
    "\n",
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "        This function does not need to be modified\n",
    "        Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: gym.core.Environment\n",
    "        Environment to play on. Must have nS, nA, and P as\n",
    "        attributes.\n",
    "        Policy: np.array of shape [env.nS]\n",
    "        The action to take at a given state\n",
    "    \"\"\"\n",
    "\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(0.25)\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "    env.render();\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "    env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    # env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_pi, 100)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_vi, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "several-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.18.0.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from gym) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.8/site-packages (from gym) (1.19.5)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow<=7.2.0\n",
      "  Downloading Pillow-7.2.0-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656451 sha256=8e63963711abc56d7828e94a35c8a6fa9005493a51efa85d7983a00d55a6bc94\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/e7/68/a3f0f1b5831c9321d7523f6fd4e0d3f83f2705a1cbd5daaa79\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, Pillow, gym\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.1.2\n",
      "    Uninstalling Pillow-8.1.2:\n",
      "      Successfully uninstalled Pillow-8.1.2\n",
      "Successfully installed Pillow-7.2.0 gym-0.18.0 pyglet-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
